import argparse
import os
import glob
import datetime as dt

import numpy as np
import pandas as pd
import yfinance as yf
import sys


def load_signals(signals_dir: str) -> pd.DataFrame:
    """Load all signals_*.csv generated by offline_parity_v3."""
    files = sorted(glob.glob(os.path.join(signals_dir, "signals_*.csv")))
    if not files:
        raise SystemExit(f"No signals_*.csv files found in {signals_dir}")

    dfs = []
    for f in files:
        try:
            x = pd.read_csv(f)
            # derive date from filename if QUOTE_DATE missing
            if "QUOTE_DATE" not in x.columns:
                base = os.path.basename(f)
                date_str = base.replace("signals_", "").replace(".csv", "")
                x["QUOTE_DATE"] = date_str
            dfs.append(x)
        except Exception as e:
            print(f"[warn] failed reading {f}: {e}")
    df = pd.concat(dfs, ignore_index=True)
    df["QUOTE_DATE"] = pd.to_datetime(df["QUOTE_DATE"], errors="coerce").dt.date
    return df


def get_prices(ticker: str, start: dt.date, end: dt.date) -> pd.Series:
    """Download Adj Close prices using yfinance."""
    # Request without auto-adjust to preserve 'Adj Close' column when possible
    try:
        data = yf.download(ticker, start=start, end=end, progress=False, auto_adjust=False)
    except Exception as e:
        raise RuntimeError(f"Failed to download prices for '{ticker}': {e}")

    # data may be a Series (single-column) or DataFrame. Try to extract adj close robustly.
    if isinstance(data, pd.Series):
        s = data
    else:
        # prefer 'Adj Close' if present
        if "Adj Close" in data.columns:
            s = data["Adj Close"]
        elif "Close" in data.columns:
            s = data["Close"]
        else:
            # handle MultiIndex columns (e.g., when yfinance returns columns like ("AAPL","Adj Close"))
            if isinstance(data.columns, pd.MultiIndex):
                # find second-level columns named 'Adj Close' or 'Close'
                second_level = data.columns.get_level_values(1)
                if "Adj Close" in second_level:
                    cols = [c for c in data.columns if c[1] == "Adj Close"]
                    # if multiple tickers present, try to pick the requested ticker
                    if len(cols) == 1:
                        s = data[cols[0]]
                    else:
                        # prefer a column whose first level matches ticker
                        match = [c for c in cols if str(c[0]).upper() == str(ticker).upper()]
                        s = data[match[0]] if match else data[cols[0]]
                elif "Close" in second_level:
                    cols = [c for c in data.columns if c[1] == "Close"]
                    match = [c for c in cols if str(c[0]).upper() == str(ticker).upper()]
                    s = data[match[0]] if match else data[cols[0]]
                else:
                    # fallback: take the first numeric column
                    s = data.iloc[:, 0]
            else:
                # fallback: take the first column
                s = data.iloc[:, 0]

    # ensure a Series and drop NA
    if isinstance(s, pd.DataFrame):
        # if multiple columns remain, take the first
        s = s.iloc[:, 0]
    s = s.dropna()
    # ensure datetime index
    s.index = pd.to_datetime(s.index)
    # return as Series
    if s.empty:
        # no price data found for the ticker/range
        raise RuntimeError(f"No price data found for ticker '{ticker}' in range {start} to {end}. Please check the ticker symbol and your network connection.")
    return s


def forward_return(px: pd.Series, date: dt.date, holding_days: int) -> float:
    """Compute forward holding-day return starting from given date."""
    dates = px.index.date
    if date not in dates:
        return np.nan
    idx = list(dates).index(date)
    j = idx + holding_days
    if j >= len(px):
        return np.nan
    p0, p1 = float(px.iloc[idx]), float(px.iloc[j])
    return (p1 - p0) / p0


def main():
    ap = argparse.ArgumentParser(description="EOD backtest for Kaggle parity signals")
    ap.add_argument("--signals_dir", type=str, default="signals")
    ap.add_argument("--ticker", type=str, required=True)
    ap.add_argument("--holding", type=int, default=1, help="Holding period in trading days")
    args = ap.parse_args()

    df = load_signals(args.signals_dir)
    ticker = args.ticker.upper()

    start = pd.Timestamp(min(df["QUOTE_DATE"])) - pd.Timedelta(days=5)
    end = pd.Timestamp(max(df["QUOTE_DATE"])) + pd.Timedelta(days=args.holding + 5)
    px = get_prices(ticker, start=start, end=end)

    # compute forward returns
    df["fwd_ret"] = [forward_return(px, d, args.holding) for d in df["QUOTE_DATE"]]

    # summarize results
    summ = (
        df.dropna(subset=["fwd_ret"])
          .groupby(["label", "side"])
          .agg(
              n=("fwd_ret", "count"),
              hit=("fwd_ret", lambda s: float((s > 0).mean())),
              mean_ret=("fwd_ret", "mean"),
              median_ret=("fwd_ret", "median"),
              p75=("fwd_ret", lambda s: float(np.percentile(s, 75))),
              p25=("fwd_ret", lambda s: float(np.percentile(s, 25))),
          )
          .reset_index()
          .sort_values("mean_ret", ascending=False)
    )

    out = os.path.join(args.signals_dir, f"bt_summary_{ticker}_h{args.holding}.csv")
    summ.to_csv(out, index=False)
    print("\nBacktest summary:")
    print(summ.to_string(index=False))
    print(f"\nSaved summary to {out}")


if __name__ == "__main__":
    main()